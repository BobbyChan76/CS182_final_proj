{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Iv03HMt3ZnZ0","executionInfo":{"status":"ok","timestamp":1700559099851,"user_tz":480,"elapsed":29222,"user":{"displayName":"Milad Shafaie","userId":"15913301691341109036"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"db8b6afa-23ad-4f99-f274-fd16a89ccc47"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd drive/MyDrive/CS\\ 182\\ Final\\ Project/Data\\ Processing/Unmodified\\ txt\\ Files"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"89m-EpqvehuS","executionInfo":{"status":"ok","timestamp":1700559105620,"user_tz":480,"elapsed":473,"user":{"displayName":"Milad Shafaie","userId":"15913301691341109036"}},"outputId":"44801b81-effd-494b-ce86-731ddda3c9d7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CS 182 Final Project/Data Processing/Unmodified txt Files\n"]}]},{"cell_type":"code","source":["!pip install striprtf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sYVxY6Xbgd7g","executionInfo":{"status":"ok","timestamp":1700559117443,"user_tz":480,"elapsed":10832,"user":{"displayName":"Milad Shafaie","userId":"15913301691341109036"}},"outputId":"31f452a9-b18a-4623-e391-10fa663b89a5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting striprtf\n","  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n","Installing collected packages: striprtf\n","Successfully installed striprtf-0.0.26\n"]}]},{"cell_type":"code","source":["from striprtf.striprtf import rtf_to_text\n","\n","with open(\"182/Lec4.rtf\") as infile:\n","    content = infile.read()\n","    text = rtf_to_text(content)\n","print(text[:1000])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n-wMq0ujfnwI","executionInfo":{"status":"ok","timestamp":1700559119660,"user_tz":480,"elapsed":988,"user":{"displayName":"Milad Shafaie","userId":"15913301691341109036"}},"outputId":"73efe615-05a2-4f78-e821-12a01d451cd6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\\documentclass[10pt]{article}\n","\\usepackage[utf8]{inputenc}\n","\\usepackage[T1]{fontenc}\n","\\usepackage{amsmath}\n","\\usepackage{amsfonts}\n","\\usepackage{amssymb}\n","\\usepackage[version=4]{mhchem}\n","\\usepackage{stmaryrd}\n","\\usepackage{bbold}\n","\\usepackage{graphicx}\n","\\usepackage[export]{adjustbox}\n","\\graphicspath{ {./images/} }\n","\n","\\title{Lecture 4: Basic Principles Part II }\n","\n","\\author{}\n","\\date{}\n","\n","\n","\\begin{document}\n","\\maketitle\n","Instructor: Anant Sahai\n","\n","Scribe: Austin Zane\n","\n","\\section*{1 Regularization}\n","\\subsection*{1.1 Explicit Regularization}\n","This is a recap of the previous lecture. For the ordinary least squares problem $\\boldsymbol{X} \\boldsymbol{w} \\approx \\boldsymbol{y}$, the solution is given by\n","\n","$$\n","\\hat{\\boldsymbol{w}}=\\left(\\boldsymbol{X}^{\\top} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\top} \\boldsymbol{y}\n","$$\n","\n","However, this can give us very large and not very useful values for the parameter vector. In such cases, it is often beneficial to \"regularize\" the parameters when training so that they stay reasonable. A c\n"]}]},{"cell_type":"code","source":["import regex as re"],"metadata":{"id":"ZmLF1UlifnlA","executionInfo":{"status":"ok","timestamp":1700559415025,"user_tz":480,"elapsed":12,"user":{"displayName":"Milad Shafaie","userId":"15913301691341109036"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["img_pattern = r'\\\\begin\\{center\\}\\n[^\\n]*\\n\\\\end\\{center\\}'"],"metadata":{"id":"X5VuotBqfnPR","executionInfo":{"status":"ok","timestamp":1700561405392,"user_tz":480,"elapsed":208,"user":{"displayName":"Milad Shafaie","userId":"15913301691341109036"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["re.findall(img_pattern, text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XqeFnpEyPXys","executionInfo":{"status":"ok","timestamp":1700561407264,"user_tz":480,"elapsed":295,"user":{"displayName":"Milad Shafaie","userId":"15913301691341109036"}},"outputId":"c148e61b-1a25-4732-a2e9-fbecf5dd104c"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2023_11_18_332723200f69b6f059ecg-5(1)}\\n\\\\end{center}',\n"," '\\\\begin{center}\\n\\\\includegraphics[max width=\\\\textwidth]{2023_11_18_332723200f69b6f059ecg-5}\\n\\\\end{center}']"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["re.sub(img_pattern, '', text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":174},"id":"jzmhu2LvPXoD","executionInfo":{"status":"ok","timestamp":1700559824920,"user_tz":480,"elapsed":246,"user":{"displayName":"Milad Shafaie","userId":"15913301691341109036"}},"outputId":"46cfd799-b0cd-4651-ba36-7132f79e07d1"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\\\documentclass[10pt]{article}\\n\\\\usepackage[utf8]{inputenc}\\n\\\\usepackage[T1]{fontenc}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage[version=4]{mhchem}\\n\\\\usepackage{stmaryrd}\\n\\\\usepackage{bbold}\\n\\\\usepackage{graphicx}\\n\\\\usepackage[export]{adjustbox}\\n\\\\graphicspath{ {./images/} }\\n\\n\\\\title{Lecture 4: Basic Principles Part II }\\n\\n\\\\author{}\\n\\\\date{}\\n\\n\\n\\\\begin{document}\\n\\\\maketitle\\nInstructor: Anant Sahai\\n\\nScribe: Austin Zane\\n\\n\\\\section*{1 Regularization}\\n\\\\subsection*{1.1 Explicit Regularization}\\nThis is a recap of the previous lecture. For the ordinary least squares problem $\\\\boldsymbol{X} \\\\boldsymbol{w} \\\\approx \\\\boldsymbol{y}$, the solution is given by\\n\\n$$\\n\\\\hat{\\\\boldsymbol{w}}=\\\\left(\\\\boldsymbol{X}^{\\\\top} \\\\boldsymbol{X}\\\\right)^{-1} \\\\boldsymbol{X}^{\\\\top} \\\\boldsymbol{y}\\n$$\\n\\nHowever, this can give us very large and not very useful values for the parameter vector. In such cases, it is often beneficial to \"regularize\" the parameters when training so that they stay reasonable. A common choice is ridge regularization, which uses a modified version of the least-squares loss function:\\n\\n$$\\n\\\\underset{\\\\boldsymbol{w}}{\\\\arg \\\\min }\\\\|\\\\boldsymbol{X} \\\\boldsymbol{w}-\\\\boldsymbol{y}\\\\|_{2}^{2}+\\\\lambda\\\\|\\\\boldsymbol{w}\\\\|_{2}^{2}\\n$$\\n\\nA bit of vector calculus shows that the new solution for $\\\\hat{\\\\boldsymbol{w}}$ is given by\\n\\n$$\\n\\\\hat{\\\\boldsymbol{w}}=\\\\left(\\\\boldsymbol{X}^{\\\\top} \\\\boldsymbol{X}+\\\\lambda \\\\boldsymbol{I}\\\\right)^{-1} \\\\boldsymbol{X}^{\\\\top} \\\\boldsymbol{y}=\\\\boldsymbol{X}^{\\\\top}\\\\left(\\\\boldsymbol{X} \\\\boldsymbol{X}^{\\\\top}+\\\\lambda \\\\boldsymbol{I}\\\\right)^{-1} \\\\boldsymbol{y}\\n$$\\n\\nwhere the first formula is the classic ridge form and the second is kernel ridge form. The equivalence can be seen as follows,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\left(\\\\boldsymbol{X}^{\\\\top} \\\\boldsymbol{X}+\\\\lambda \\\\boldsymbol{I}\\\\right)^{-1} \\\\boldsymbol{X}^{\\\\top} \\\\boldsymbol{y} & =\\\\left(\\\\boldsymbol{X}^{\\\\top} \\\\boldsymbol{X}+\\\\lambda \\\\boldsymbol{I}\\\\right)^{-1} \\\\boldsymbol{X}^{\\\\top}\\\\left(\\\\boldsymbol{X} \\\\boldsymbol{X}^{\\\\top}+\\\\lambda \\\\boldsymbol{I}\\\\right)\\\\left(\\\\boldsymbol{X} \\\\boldsymbol{X}^{\\\\top}+\\\\lambda \\\\boldsymbol{I}\\\\right)^{-1} \\\\boldsymbol{y} \\\\\\\\\\n& =\\\\left(\\\\boldsymbol{X}^{\\\\top} \\\\boldsymbol{X}+\\\\lambda \\\\boldsymbol{I}\\\\right)^{-1}\\\\left(\\\\boldsymbol{X}^{\\\\top} \\\\boldsymbol{X} \\\\boldsymbol{X}^{\\\\top}+\\\\lambda \\\\boldsymbol{X}^{\\\\top}\\\\right)\\\\left(\\\\boldsymbol{X} \\\\boldsymbol{X}^{\\\\top}+\\\\lambda \\\\boldsymbol{I}\\\\right)^{-1} \\\\boldsymbol{y} \\\\\\\\\\n& =\\\\left(\\\\boldsymbol{X}^{\\\\top} \\\\boldsymbol{X}+\\\\lambda \\\\boldsymbol{I}\\\\right)^{-1}\\\\left(\\\\boldsymbol{X}^{\\\\top} \\\\boldsymbol{X}+\\\\lambda \\\\boldsymbol{I}\\\\right) \\\\boldsymbol{X}^{\\\\top}\\\\left(\\\\boldsymbol{X} \\\\boldsymbol{X}^{\\\\top}+\\\\lambda \\\\boldsymbol{I}\\\\right)^{-1} \\\\boldsymbol{y} \\\\\\\\\\n& =\\\\boldsymbol{X}^{\\\\top}\\\\left(\\\\boldsymbol{X} \\\\boldsymbol{X}^{\\\\top}+\\\\lambda \\\\boldsymbol{I}\\\\right)^{-1} \\\\boldsymbol{y}\\n\\\\end{aligned}\\n$$\\n\\nThere is a way of thinking of the first formulation as solving the primal and the second as solving the dual.\\n\\n\\\\subsection*{1.2 Data and Feature Augmentation}\\nInstead of explicitly changing the loss function, we can add $d$ \"fake\" data points to our data matrix to achieve the same regularizing effect (as proven in previous lecture):\\n\\n$$\\n\\\\left[\\\\begin{array}{c}\\n\\\\boldsymbol{X} \\\\\\\\\\n\\\\sqrt{\\\\lambda} \\\\\\\\\\n\\\\boldsymbol{I}_{d}\\n\\\\end{array}\\\\right] \\\\boldsymbol{w} \\\\approx\\\\left[\\\\begin{array}{c}\\n\\\\boldsymbol{y} \\\\\\\\\\n\\\\mathbf{0}_{d}\\n\\\\end{array}\\\\right]\\n$$\\n\\nwhere the new data matrix is in $\\\\mathbb{R}^{(n+d) \\\\times d}$ and the new response is a vector in $\\\\mathbb{R}^{n+d}$. This is an important concept and can be thought of as improving the conditioning number of the data matrix. Plugging the new data matrix and response vector into the classic OLS solution immediately gives us the solution for the ridge regularized problem.\\n\\nAnother equivalent option is adding $n$ fake features:\\n\\n$$\\n\\\\left[\\\\begin{array}{ll}\\n\\\\boldsymbol{X} & \\\\sqrt{\\\\lambda} \\\\boldsymbol{I}_{n}\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{l}\\n\\\\boldsymbol{w} \\\\\\\\\\n\\\\boldsymbol{f}\\n\\\\end{array}\\\\right]=\\\\boldsymbol{y}\\n$$\\n\\nWe use the Moore-Penrose pseudoinverse to solve for the new weight vector,\\n\\n$$\\n\\\\left[\\\\begin{array}{c}\\n\\\\boldsymbol{w} \\\\\\\\\\n\\\\boldsymbol{f}\\n\\\\end{array}\\\\right]=\\\\left[\\\\begin{array}{ll}\\n\\\\boldsymbol{X} & \\\\sqrt{\\\\lambda} \\\\boldsymbol{I}_{n}\\n\\\\end{array}\\\\right]^{\\\\dagger} \\\\boldsymbol{y}=\\\\left[\\\\begin{array}{c}\\n\\\\boldsymbol{X}^{\\\\top} \\\\\\\\\\n\\\\sqrt{\\\\lambda} \\\\boldsymbol{I}_{n}\\n\\\\end{array}\\\\right]\\\\left(\\\\left[\\\\begin{array}{ll}\\n\\\\boldsymbol{X} & \\\\sqrt{\\\\lambda} \\\\boldsymbol{I}_{n}\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{c}\\n\\\\boldsymbol{X}^{\\\\top} \\\\\\\\\\n\\\\sqrt{\\\\lambda} \\\\boldsymbol{I}_{n}\\n\\\\end{array}\\\\right]\\\\right)^{-1} \\\\boldsymbol{y}\\n$$\\n\\nWe are only interested in solving for $\\\\boldsymbol{w}$, so we disregard the $n$ rows corresponding to the false parameter $\\\\boldsymbol{f}$. In the end, we are left with the minimum norm solution to the under-determined system specified above,\\n\\n$$\\n\\\\boldsymbol{w}=\\\\boldsymbol{X}^{\\\\top}\\\\left(\\\\left[\\\\begin{array}{ll}\\n\\\\boldsymbol{X} & \\\\sqrt{\\\\lambda} \\\\boldsymbol{I}_{n}\\n\\\\end{array}\\\\right]\\\\left[\\\\begin{array}{c}\\n\\\\boldsymbol{X}^{\\\\top} \\\\\\\\\\n\\\\sqrt{\\\\lambda} \\\\boldsymbol{I}_{n}\\n\\\\end{array}\\\\right]\\\\right)^{-1} \\\\boldsymbol{y}=\\\\boldsymbol{X}^{\\\\top}\\\\left(\\\\boldsymbol{X} \\\\boldsymbol{X}^{\\\\top}+\\\\lambda \\\\boldsymbol{I}\\\\right)^{-1} \\\\boldsymbol{y}\\n$$\\n\\n\\\\subsection*{1.3 Using Singular Value Decomposition to Simplify Regularization}\\nUsing the singular value decomposition (SVD) in place of $\\\\boldsymbol{X}$ in the above equations allows us to simplify things and make the algorithms far easier to handle. Namely, we are able to update each weight individually instead of solving systems of equations. This can be seen by taking the SVD of $\\\\boldsymbol{X}$ and solving the unregularized problem\\n\\n$$\\n\\\\begin{aligned}\\n\\\\boldsymbol{X} \\\\boldsymbol{w}=\\\\boldsymbol{U} \\\\boldsymbol{\\\\Sigma} \\\\boldsymbol{V}^{\\\\top} \\\\boldsymbol{w} & \\\\approx \\\\boldsymbol{y} \\\\\\\\\\n\\\\Rightarrow \\\\boldsymbol{\\\\Sigma} \\\\widetilde{\\\\boldsymbol{w}} & \\\\approx \\\\widetilde{\\\\boldsymbol{y}}\\n\\\\end{aligned}\\n$$\\n\\nwhere $\\\\widetilde{\\\\boldsymbol{w}}:=\\\\boldsymbol{V}^{\\\\top} \\\\boldsymbol{w}, \\\\widetilde{\\\\boldsymbol{y}}:=\\\\boldsymbol{U}^{\\\\top} \\\\boldsymbol{y}$, and $\\\\boldsymbol{\\\\Sigma} \\\\in \\\\mathbb{R}^{n \\\\times d}$. Note that $\\\\boldsymbol{U}, \\\\boldsymbol{V}$ are orthogonal matrices so they merely rotate our vectors while preserving their norms. Let $\\\\sigma_{i}$ denote the $i^{\\\\text {th }}$ singular value. Because $\\\\boldsymbol{\\\\Sigma}$ is diagonal and we are assuming that $n>d$, only the first $d$ equations will be meaningful here:\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\text { for } d \\\\text { equations: } \\\\sigma_{i} \\\\widetilde{\\\\boldsymbol{w}}[i] \\\\approx \\\\widetilde{\\\\boldsymbol{y}}[i] \\\\Rightarrow \\\\widetilde{\\\\boldsymbol{w}}[i] \\\\approx \\\\frac{1}{\\\\sigma_{i}} \\\\widetilde{\\\\boldsymbol{y}}[i] \\\\\\\\\\n& \\\\text { for } n-d \\\\text { equations: } 0 \\\\approx \\\\widetilde{\\\\boldsymbol{y}}[i]\\n\\\\end{aligned}\\n$$\\n\\nAs previously stated, using these coordinates removes the system of equations and allows us to simply solve for the individual weight components. Observe that we are dividing by the singular values so problems may arise if they become too small (i.e. if the matrix is ill-conditioned). This is the underlying cause of OLS sometimes giving us wild values.\\n\\nNext, we consider using ridge regression. In this setting, the solution is obtained by plugging the SVD of $\\\\boldsymbol{X}$\\ninto the previously mentioned classic solution for ridge regression.\\n\\n$$\\n\\\\begin{aligned}\\n\\\\hat{\\\\boldsymbol{w}} & =\\\\left(\\\\boldsymbol{V} \\\\boldsymbol{\\\\Sigma}^{\\\\top} \\\\boldsymbol{\\\\Sigma} \\\\boldsymbol{V}^{\\\\top}+\\\\lambda \\\\boldsymbol{I}\\\\right)^{-1} \\\\boldsymbol{V} \\\\boldsymbol{\\\\Sigma}^{\\\\top} \\\\boldsymbol{U}^{\\\\top} \\\\boldsymbol{y} \\\\\\\\\\n& =\\\\boldsymbol{V}\\\\left(\\\\boldsymbol{\\\\Sigma}^{\\\\top} \\\\boldsymbol{\\\\Sigma}+\\\\lambda \\\\boldsymbol{I}\\\\right)^{-1} \\\\boldsymbol{V}^{\\\\top} \\\\boldsymbol{V} \\\\boldsymbol{\\\\Sigma}^{\\\\top} \\\\widetilde{\\\\boldsymbol{y}} \\\\\\\\\\n& =\\\\boldsymbol{V}\\\\left(\\\\boldsymbol{\\\\Sigma}^{\\\\top} \\\\boldsymbol{\\\\Sigma}+\\\\lambda \\\\boldsymbol{I}\\\\right)^{-1} \\\\boldsymbol{\\\\Sigma}^{\\\\top} \\\\widetilde{\\\\boldsymbol{y}} \\\\\\\\\\n\\\\Rightarrow \\\\widehat{\\\\widetilde{\\\\boldsymbol{w}}} & =\\\\left(\\\\boldsymbol{\\\\Sigma}^{\\\\top} \\\\boldsymbol{\\\\Sigma}+\\\\lambda \\\\boldsymbol{I}\\\\right)^{-1} \\\\boldsymbol{\\\\Sigma}^{\\\\top} \\\\widetilde{\\\\boldsymbol{y}} .\\n\\\\end{aligned}\\n$$\\n\\nIn the end, we are left with solutions of the form\\n\\n$$\\n\\\\widetilde{\\\\boldsymbol{w}}[i]=\\\\frac{\\\\sigma_{i}}{\\\\sigma_{i}^{2}+\\\\lambda} \\\\widetilde{\\\\boldsymbol{y}}[i]\\n$$\\n\\nIf $\\\\lambda<<\\\\sigma_{i}^{2}$, then we are in the same situation as the unregularized case. If $\\\\lambda>>\\\\sigma_{i}^{2}$, then the weights are forced to stay small instead of behaving wildly.\\n\\n\\\\subsection*{1.4 Implicit Regularization}\\nImplicit regularization is the regularization that occurs when we aren\\'t consciously doing any regularization. When performing explicit regularization as above, we must specify a specific regularization hyperparameter and modify the training data, model architecture, or loss function. In contrast, implicit regularization is an unexpected benefit stemming from our choice of optimizer. We will see that choosing gradient descent as our optimization algorithm, combined with the large size of DNNs, provides enough regularization for DNNs to generalize well without us intentionally restricting the parameters.\\n\\nTo gain intuition, let\\'s look at gradient descent (GD) updates for OLS in SVD coordinates:\\n\\n$$\\n\\\\widetilde{\\\\boldsymbol{w}}_{t+1}=\\\\widetilde{\\\\boldsymbol{w}}_{t}+2 \\\\eta \\\\boldsymbol{\\\\Sigma}^{\\\\top}\\\\left(\\\\widetilde{\\\\boldsymbol{y}}-\\\\boldsymbol{\\\\Sigma} \\\\widetilde{\\\\boldsymbol{w}}_{t}\\\\right)\\n$$\\n\\nwhere $\\\\widetilde{\\\\boldsymbol{w}}_{t}$ represents the parameter vector at the current step, $\\\\widetilde{\\\\boldsymbol{w}}_{t+1}$ represents the updated parameter vector, $\\\\boldsymbol{\\\\Sigma}$ represents the diagonal matrix of singular values from the above SVD, $\\\\widetilde{\\\\boldsymbol{y}}:=\\\\boldsymbol{U}^{\\\\top} \\\\boldsymbol{y}$ as defined above, and $\\\\eta$ is our learning rate hyperparameter.\\n\\nNote that because we are dealing with a diagonal matrix, this works out to updating each component of the weight vector individually. They don\\'t interact with each other at all during GD, so each is being modified as follows:\\n\\n$$\\n\\\\widetilde{\\\\boldsymbol{w}}_{t+1}[i]=\\\\widetilde{\\\\boldsymbol{w}}_{t}[i]+2 \\\\eta \\\\sigma_{i}\\\\left(\\\\widetilde{\\\\boldsymbol{y}}[i]-\\\\sigma_{i} \\\\widetilde{\\\\boldsymbol{w}}_{t}[i]\\\\right)\\n$$\\n\\nThis is potentially unstable because we are not reducing $\\\\widetilde{\\\\boldsymbol{w}}_{t}[i]$ at each step as we did in the last lecture. This means that, subject to a bounded input, we might get an unbounded output if we allow the algorithm to run forever. Observe that the stationary point is the solution we discussed earlier, $\\\\widetilde{\\\\boldsymbol{w}}[i]=\\\\frac{1}{\\\\sigma_{i}} \\\\widetilde{\\\\boldsymbol{y}}[i]$. If $\\\\sigma_{i}$ is tiny, then we find ourselves in a bad situation.\\n\\nLet\\'s carefully calculate the first few steps of GD to see what\\'s going on:\\n\\n$$\\n\\\\begin{aligned}\\n& \\\\widetilde{\\\\boldsymbol{w}}_{0}[i]=0 \\\\\\\\\\n& \\\\widetilde{\\\\boldsymbol{w}}_{1}[i]=2 \\\\eta \\\\sigma_{i} \\\\widetilde{\\\\boldsymbol{y}}[i] \\\\\\\\\\n& \\\\widetilde{\\\\boldsymbol{w}}_{2}[i]=2 \\\\eta \\\\sigma_{i} \\\\widetilde{\\\\boldsymbol{y}}[i]+2 \\\\eta \\\\sigma_{i}\\\\left(\\\\widetilde{\\\\boldsymbol{y}}[i]-\\\\sigma_{i} 2 \\\\eta \\\\sigma_{i} \\\\widetilde{\\\\boldsymbol{y}}[i]\\\\right) \\\\approx 4 \\\\eta \\\\sigma_{i} \\\\widetilde{\\\\boldsymbol{y}}[i]\\n\\\\end{aligned}\\n$$\\n\\nObserve that this is roughly a linear function with an extremely small slope if $\\\\sigma_{i}$ is tiny. In this situation, GD barely moves in the early stages even though it will eventually converge to a very large value, as previously discussed. Together with early stopping, this means that GD is trying to do something like ridge regularization for us because it will resist enlarging the directions corresponding to small singular values. Early stopping is when we stop the training process because validation performance has gotten worse or has not improved for a long time. It is important to note that GD, when initialized at zero, will converge to the minimum-norm solution. This is a good exercise for the reader to verify.\\n\\nTo summarize, there are three kinds of regularization: explicit regularization, data augmentation (adding fake observations or features), and implicit regularization (optimizer has implicit regularizing effect). Regarding DNNs, the combination of the min-norm seeking behavior of gradient descent and the feature augmentation that is implicit when using large networks gives a lot of regularization even if we weren\\'t thinking about it.\\n\\n\\\\section*{2 Trade-offs Between Qualitatively Different Sources of Error}\\nSuppose we have learned a model $\\\\hat{\\\\theta} \\\\rightarrow f_{\\\\hat{\\\\theta}}$. At \"test time\", we look at the error, $\\\\left(Y_{\\\\text {observed }}-f_{\\\\hat{\\\\theta}}(x)\\\\right)$. There are three main sources of error:\\n\\n\\\\begin{enumerate}\\n  \\\\item Irreducible error: This is due to noise or randomness from $Y \\\\mid X$ itself. In short, there is some level of noise that is impossible for our model to account for. One possible situation is that the underlying response is a deterministic function of $X$ (i.e. not random), but there is randomness in the measurement. It is possible that our model perfectly predicts the underlying signal, but the model will still disagree with $Y_{\\\\text {observed }}$ and contribute to the error. In the classic setup of $y=f_{\\\\text {true }}(x)+\\\\varepsilon_{\\\\text {noise }}$, the $\\\\varepsilon_{\\\\text {noise }}$ term contributes to irreducible error.\\n\\n  \\\\item Approximation error: This error comes from limited expressive power of $f_{\\\\theta}$ as a finitely-parameterized model. In other words, our model isn\\'t \"flexible\" enough to capture the true signal. For example, trying to fit the function $y=\\\\cos (x)$ using a single 6 th degree polynomial model, $f_{\\\\theta}(x)=\\\\sum_{i=0}^{6} a_{i} x^{i}$.\\n\\n  \\\\item Estimation error: There are two components to this type of error, both of which are well-covered in prerequisite machine learning courses:\\n\\n\\\\end{enumerate}\\n\\n(a) Bias: Bias captures the systematic error of our learning algorithm and training data in terms of making predictions. We write this mathematically by $\\\\mathbb{E}_{\\\\boldsymbol{\\\\varepsilon}, \\\\mathcal{D}}\\\\left[f_{\\\\hat{\\\\theta}(\\\\mathcal{D})}(X)-Y \\\\mid X\\\\right]$. Note that $\\\\varepsilon$ respresents the randomness in $Y \\\\mid X$ and $\\\\mathcal{D}$ represents the randomness in the training process used to get $\\\\hat{\\\\theta}$. For example, this could include the training data set we used or the splits made in random forest trees. Traditionally, $\\\\mathrm{X}$ is separate from $\\\\mathcal{D}$ and is not random. It is common to look at the squared bias to prevent positive and negative biases for different observations from canceling.\\n\\n(b) Variance: This is the variable part of error. We write it as $\\\\mathbb{E}_{\\\\mathcal{D}}\\\\left[\\\\left(f_{\\\\hat{\\\\theta}(\\\\mathcal{D})}(X)-\\\\mathbb{E}_{\\\\mathcal{D}}\\\\left[f_{\\\\hat{\\\\theta}(\\\\mathcal{D})}(X)\\\\right]\\\\right)^{2} \\\\mid X\\\\right]$, where $\\\\mathcal{D}$ again represents the randomness of the training process. It describes how much our prediction \"shakes\" as a function of the randomness in the training.\\n\\nThis perspective can be useful and many papers utilize the bias-variance decomposition in their derivations. However, it doesn\\'t always match what our intuition might be, especially in deep learning settings. The following figure is intended to help us understand this point.\\n\\nThis drawing is for high-level intuition, so we musn\\'t allow ourselves to become confused over the exact dimensions and projections. We first turn our attention to the figure on the left. The line passing through\\n\\n\\n\\nFigure 1: Approximation and Estimation Error\\n\\nthe origin represents the subspace models that we are considering. Note that we are only capable of providing estimates that fall along this line. As we can see, the true model is not in this subspace so we will suffer a certain amount of approximation error.\\n\\nTurning our attention to the figure on the right, we project the true model onto the subspace of models that we are considering. The result is the best possible approximation and is the goal of our learning algorithm. We see in the figure that our predicted model is not quite equal to the best approximation. In this perspective, instead of thinking of things in terms of bias and variance, we consider how much of the \"true\" and \"false\" directions we are incorporating into our prediction.\\n\\nWe make this a bit more formal by discussing survival and contamination. Survival reflects, in expectation, how much of the true pattern survives the estimation/learning process. Contamination is how much useless information get \"learned\", e.g. spurious features that our model is capable of picking up but don\\'t help with prediction. Survival and contamination can be thought of as the intuition behind bias and variance, respectively.\\n\\n\\\\section*{3 What are \"Features\"?}\\nThe following is a simplified sketch of a neural network with $\\\\ell$ hidden layers.\\n\\n\\n\\nFigure 2: Simplified DNN\\n\\nIf we treat everything before the output of $H_{\\\\ell}$ as a black box, we can think of things from the perspective of a generalized linear model. We have a featurizer, some linear function of those new features, and a loss that we are optimizing. The featurizer lifts or distills the input $\\\\mathrm{X}$ into a nicer feature space. In this perspective, the \"learned\" features are the outputs of the penultimate layer in the featurizer, $H_{\\\\ell}$. We want the featurization to be data-driven instead of hand-picked, so the layers essentially find a representation of the data that allows the generalized linear model (GLM) to work well.\\n\\nHowever, there is another important point of view. Suppose a generalized linear model is given by $\\\\hat{y}=$ $\\\\sum_{i=1}^{\\\\lambda} w_{i} \\\\phi_{i}(\\\\boldsymbol{x})$, where $\\\\boldsymbol{\\\\phi}(\\\\boldsymbol{x})$ is the output of the \"featurizer\" in the figure above. When we are actually using the model on new data, these are simply the features. However, from the training perspective, they also determine the gradient. The derivative of the linear model with respect to the $i^{t h}$ parameter is $\\\\frac{\\\\partial \\\\hat{y}}{\\\\partial w_{i}}=\\\\phi_{i}(\\\\boldsymbol{x})$.\\n\\nIn short, we don\\'t understand nonlinear systems well, so our standard approach to understanding a nonlinear system is local linearization. We zoom in until things are roughly linear around a certain point. In terms of DNNs, we say that the deep network is Taylor expanded around the features in such a way that it is some constant term plus a local GLM in which small increments of the features change the predictions in a small way. This will be covered in greater detail during the next lecture.\\n\\n\\n\\\\end{document}'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["cleaned_text = re.sub(img_pattern, '', text)"],"metadata":{"id":"g76QSQ-2PXcV","executionInfo":{"status":"ok","timestamp":1700561412244,"user_tz":480,"elapsed":231,"user":{"displayName":"Milad Shafaie","userId":"15913301691341109036"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["metadata_patterns = [r'\\\\documentclass[^\\n]*', r'\\\\usepackage[^\\n]*', r'\\\\graphicspath[^\\n]*', r'\\\\end\\{document}']"],"metadata":{"id":"CWFUymAbWE_0","executionInfo":{"status":"ok","timestamp":1700561414228,"user_tz":480,"elapsed":211,"user":{"displayName":"Milad Shafaie","userId":"15913301691341109036"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["for pattern in metadata_patterns:\n","  cleaned_text = re.sub(pattern, '', cleaned_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V1MUijK4Vdz2","executionInfo":{"status":"ok","timestamp":1700561415724,"user_tz":480,"elapsed":192,"user":{"displayName":"Milad Shafaie","userId":"15913301691341109036"}},"outputId":"bb52b96b-35b2-49f5-f130-3b1bab0faedc"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["\\\\documentclass[^\\n]*\n","\\\\usepackage[^\\n]*\n","\\\\graphicspath[^\\n]*\n","\\\\end\\{document}\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"RZKxzga5VsKy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd .."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hV1XzayDRaAJ","executionInfo":{"status":"ok","timestamp":1700559969527,"user_tz":480,"elapsed":170,"user":{"displayName":"Milad Shafaie","userId":"15913301691341109036"}},"outputId":"c0c9a74b-d20f-4563-a8fa-a05b455e814d"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CS 182 Final Project/Data Processing\n"]}]},{"cell_type":"code","source":["%cd Cleaned_Notes/182_cleaned"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_aglvGpR7lI","executionInfo":{"status":"ok","timestamp":1700560153006,"user_tz":480,"elapsed":215,"user":{"displayName":"Milad Shafaie","userId":"15913301691341109036"}},"outputId":"b1ccf038-11f6-4782-b24f-ed2fa2264bdd"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CS 182 Final Project/Data Processing/Cleaned_Notes/182_cleaned\n"]}]},{"cell_type":"code","source":["with open(\"Lec4.txt\", \"w\") as text_file:\n","    text_file.write(cleaned_text)"],"metadata":{"id":"QsDQZHAZRVAO","executionInfo":{"status":"ok","timestamp":1700561485557,"user_tz":480,"elapsed":369,"user":{"displayName":"Milad Shafaie","userId":"15913301691341109036"}}},"execution_count":35,"outputs":[]}]}