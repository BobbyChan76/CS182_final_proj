questions,answers
"Define the Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) approaches in the context of regression problems. How were these approaches used to justify the optimization formulations of Ordinary Least Squares (OLS) and ridge regression, respectively?
","MLE is a method for estimating the parameters of a statistical model by maximizing the likelihood function, thereby finding the parameter values that make the observed data most probable. MAP is similar to MLE but incorporates prior beliefs (prior probability distribution) about the parameter values in the estimation process, effectively updating the prior with data to obtain a posterior distribution that is then maximized. In regression problems, MLE has been used to justify the optimization formulation of OLS, which minimizes the sum of squared differences between observed and predicted values under the assumption of i.i.d Gaussian noise. Similarly, MAP has been used to justify the optimization formulation of ridge regression, which adds a regularization term corresponding to a Gaussian prior on the regression coefficients, also under the assumption that the parameters are i.i.d Gaussian.

"
"Explain the change of coordinates technique and its relevance in solving non-i.i.d problems, particularly regarding regression with non-identical or dependent noise and parameters.
","The change of coordinates technique involves transforming the data and/or parameters such that the transformed problem exhibits properties that are more manageable or desirable, such as independence and identical distribution (i.i.d). For regression problems with non-identical or dependent noise and parameters, this technique allows us to reduce the problem back to the i.i.d case, which can then be solved using the original methods employed for OLS and ridge regression. This approach involves scaling the data matrix and the observation vector appropriately to achieve a transformed problem with i.i.d noise or parameters.

"
"Define the optimization problem for weighted least squares (wLS) and explain how it differs from ordinary least squares (OLS).
","The optimization problem for weighted least squares (wLS) is given by minimizing the sum of weighted squared differences between observed and predicted values, where each term in the sum is weighted by a positive coefficient $\omega_{i}$. This is expressed as:
$$
\hat{\mathbf{w}}_{\mathrm{wLS}}=\underset{\mathbf{w} \in \mathbb{R}^{d}}{\arg \min }\left(\sum_{i=1}^{n} \omega_{i}\left(y_{i}-\mathbf{x}_{i}^{\top} \mathbf{w}\right)^{2}\right)
$$
This differs from OLS, where each term in the sum is not weighted and all data points contribute equally to the error term. The weighted least squares approach allows for emphasizing the fit on certain data points more than others, which can be useful when some data points are considered more reliable or important.

"
"In the context of weighted least squares from a probabilistic perspective, how are the weights $\omega_{i}$ related to the variances $\sigma_{i}^{2}$ of the observation noise?
","In the probabilistic model for weighted least squares, the weights $\omega_{i}$ are inversely related to the variances $\sigma_{i}^{2}$ of the observation noise. This is expressed as $\omega_{i}=\frac{1}{\sigma_{i}^{2}}$. This relationship signifies that as the variance of the noise corresponding to a data point $i$ decreases (indicating less noise and higher reliability), the weight $\omega_{i}$ increases, emphasizing the fitting of this data point in the regression. Conversely, as the variance increases (indicating more noise), the weight $\omega_{i}$ decreases, reflecting a decreased emphasis on fitting that data point.

"
"Describe the generalized least squares (GLS) problem formulation and how it differs from the ordinary least squares (OLS) problem in the presence of dependent noise variables.
","Generalized least squares (GLS) addresses the case when noise variables are not independent but are instead dependent on one another. The GLS problem formulation is to maximize the likelihood of the data over the set of possible weight vectors $\mathbf{w}$, which is equivalent to minimizing the following expression:
$$
\hat{\mathbf{w}}_{\mathrm{GLS}}=\underset{\mathbf{w} \in \mathbb{R}^{d}}{\arg \min }(\mathbf{y}-\mathbf{X} \mathbf{w})^{\top} \boldsymbol{\Sigma}_{\mathbf{Z}}^{-1}(\mathbf{y}-\mathbf{X} \mathbf{w})
$$
where $\boldsymbol{\Sigma}_{\mathbf{Z}}$ represents the covariance matrix of the noise variables. This differs from OLS, where the noise is assumed to be independent and identically distributed, and therefore, the covariance matrix of the noise is the identity matrix, leading to a simpler minimization problem without the need for inverting the covariance matrix."
"Define the convolution operator as used in Convolutional Neural Networks (CNNs) and explain the concept of weight sharing. How does weight sharing in convolutional layers compare to the fully connected (FC) architecture in neural networks?
","The convolution operator in CNNs is mathematically defined as follows:
$$
(\mathbf{I} * \mathbf{G})[x, y]=\sum_{a=0}^{w-1} \sum_{b=0}^{h-1} \sum_{c \in\{1 \cdots D\}} I_{c}[x+a, y+b] \cdot G_{c}[a, b]
$$
This expression describes the process of applying a filter (or kernel) $\mathbf{G}$ across an input $\mathbf{I}$ to produce an output $\mathbf{L}$, which is essentially a feature map. Weight sharing refers to the use of the same filter $\mathbf{G}$ for different positions of the input, which significantly reduces the number of parameters compared to a FC layer that would require separate weights for each connection between input and output neurons. In the FC architecture, for each output unit, there is a separate weight for each input-output connection. On the contrary, in convolutional layers, the same filter weights are applied across the entire input, thus sharing weights and reducing the model's complexity and its variance while retaining expressiveness.

"
"Describe the process and effect of applying a simple horizontal edge detector filter in the context of image classification using CNNs.
","A simple horizontal edge detector filter, such as [1 -1], is applied to an image to detect horizontal edges. This filter will produce large negative values when the left pixel is bright and the right pixel is dark, and conversely, it will produce large positive values when the left pixel is dark and the right pixel is bright. In the context of image classification using CNNs, such filters are used to extract low-level features like edges, which are the building blocks for detecting more complex shapes and objects in images. These features are critical for the classification of images as they provide the necessary information for the model to recognize patterns.

"
"Explain the concept of receptive field in CNNs and how stacking convolutional layers affects it. Why is this concept important for capturing complex patterns in images?
","The receptive field in CNNs refers to the size of the region in the input space that affects a particular unit's output. As more convolutional layers are stacked in a CNN, the effective receptive field for units in successive layers increases. This means that the value of any single unit in deeper layers is informed by a larger patch of the original image. For example, if two successive layers of $3 \times 3$ filters are used, units in the second convolutional layer can be informed by up to $9 \times 9 = 81$ original pixels. This increase in the receptive field allows the network to capture larger and more complex patterns as it progresses deeper, with earlier layers detecting local low-level features and later layers detecting global high-level features. This is important for recognizing complex objects and patterns in images which cannot be identified by simply looking at local features.

"
"Define max-pooling and average-pooling operations in CNNs. What is the purpose of pooling layers in neural networks?
","Max-pooling and average-pooling are two common implementations of pooling operations in CNNs. In max-pooling, the representative value in the pooled region becomes the largest of all the units in the window. In average-pooling, the representative value is the average of all the units in the window. The purpose of pooling layers is to downsample the input representation, reducing its dimensionality and allowing for assumptions to be made about features contained in the sub-regions binned. This helps to reduce the number of parameters and computation in the network, and also to make the detection of certain features somewhat invariant to scale and orientation changes.

"
"Describe the role of backpropagation in training CNNs, and outline the process of computing partial derivatives for convolutional layers.
","Backpropagation in training CNNs is used to compute the gradients of the error function with respect to the network parameters (weights) to update them and minimize the loss. For convolutional layers, this involves calculating the partial derivatives of the error with respect to the output of the layer and then using the chain rule to find the derivatives with respect to the elements in the input and the filter. The derivatives for the filter weights and the input image are computed as follows:
$$
\frac{\partial f}{\partial G_{c}[x, y]} = \sum_{i, j} \frac{\partial f}{\partial L[i, j]} I_{c}[i+x, j+y]
$$
$$
\frac{\partial f}{\partial I_{c}[x, y]} = \sum_{i, j} \frac{\partial f}{\partial L[i, j]} G_{c}[x-i, y-j]
$$
These gradients are then used to update the filter weights and improve the feature extraction capabilities of the convolutional layers. It is through this process that CNNs learn to extract relevant features from images for tasks like classification."
"Define the optimization problem that is solved in ridge regression and the issues that arise with polynomial feature augmentation. What is the proposed solution to address the computational challenges associated with high-dimensional augmented feature spaces?
","In ridge regression, the optimization problem involves finding the parameter vector $\mathbf{w} \in \mathbb{R}^{d}$ that minimizes the loss function based on a given vector $\mathbf{y} \in \mathbb{R}^{n}$ and a matrix $\mathbf{X} \in \mathbb{R}^{n \times \ell}$, where $n$ is the number of training points and $\ell$ is the dimension of the raw data points. The issue that arises with polynomial feature augmentation is that when polynomial features of degree at most $p$ are added to the raw $\ell$ dimensional space, the number of terms that need to be optimized becomes $d = \left(\begin{array}{c}\ell+p \\ p\end{array}\right)$, which can be much larger than the number of training points $n$. The proposed solution to address these computational challenges is to solve an equivalent problem over $n$ variables instead of $d$ variables, with a computational runtime independent of the number of augmented features, using the concept of kernels and the kernel trick.

"
"Define the matrix inversion lemma and apply it to derive the expression for the optimal weight vector $\mathbf{w}^*$ in kernel ridge regression.
","The matrix inversion lemma is not explicitly defined in the provided text, but the derivation uses the concept of expressing the solution to ridge regression using matrices of inner products between augmented data points, specifically the matrix $\boldsymbol{\Phi} \boldsymbol{\Phi}^{\top}$. The optimal weight vector $\mathbf{w}^*$ in kernel ridge regression is derived by substituting $\mathbf{X}^{\top} \mathbf{v}$ for $\mathbf{w}$ and finding a vector $\mathbf{v}$ such that $\mathbf{X} \mathbf{X}^{\top} \mathbf{v} + \lambda \mathbf{v} = \mathbf{y}$. This leads to the expression $\mathbf{w}^* = \mathbf{X}^{\top} (\mathbf{X} \mathbf{X}^{\top} + \lambda \mathbf{I})^{-1} \mathbf{y}$.

"
"Define the Fundamental Theorem of Linear Algebra (FTLA) and explain how it is used in the linear algebra derivation of the kernel ridge regression solution.
","The Fundamental Theorem of Linear Algebra (FTLA) states that for a matrix (linear map) $\mathbf{X}$ mapping $\mathbb{R}^{\ell}$ to $\mathbb{R}^{n}$, the nullspace $\mathcal{N}(\mathbf{X})$ and the range $\mathcal{R}(\mathbf{X})$ have the properties that $\mathcal{N}(\mathbf{X}) \stackrel{\perp}{\oplus} \mathcal{R}\left(\mathbf{X}^{\top}\right)=\mathbb{R}^{\ell}$ and $\mathcal{N}\left(\mathbf{X}^{\top}\right) \stackrel{\perp}{\oplus} \mathcal{R}(\mathbf{X})=\mathbb{R}^{n}$, among others. This theorem is used in the derivation of the kernel ridge regression solution by expressing any weight vector $\mathbf{w} \in \mathbb{R}^{\ell}$ as a unique combination $\mathbf{w}=\mathbf{w}_1 + \mathbf{w}_2$ with $\mathbf{w}_1 \in \mathcal{R}\left(\mathbf{X}^{\top}\right)$ and $\mathbf{w}_2 \in \mathcal{N}(\mathbf{X})$, which simplifies the optimization problem to finding a vector $\mathbf{v}$ that minimizes the ridge regression loss function.

"
"Define the kernel function and the Gram matrix in the context of kernel ridge regression. What are the conditions for a function to be considered a valid kernel function?
","The kernel function $k(\cdot, \cdot)$ is a function that takes raw-feature inputs and outputs their inner product in the augmented feature space, effectively measuring the similarity between the data points. The Gram matrix $\mathbf{K}$ is defined as a matrix where each entry $\mathbf{K}_{ij}$ is the kernel function applied to a pair of data points, i.e., $k(\mathbf{x}_i, \mathbf{x}_j)$. A function $k(\cdot, \cdot)$ is considered a valid kernel function if either (1) there exists a feature map $\phi(\cdot)$ such that for all $\mathbf{x}_i, \mathbf{x}_j$, $k(\mathbf{x}_i, \mathbf{x}_j) = \langle \phi(\mathbf{x}_i), \phi(\mathbf{x}_j) \rangle$, or (2) for all sets $\mathcal{D} = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n\}$, the Gram matrix $\mathbf{K}(\mathcal{D})$ is positive semidefinite.

"
"Describe the kernel trick and its significance in reducing computational complexity, especially in the context of polynomial feature maps.
","The kernel trick refers to the method of computing the inner product of data points in the augmented feature space without explicitly mapping the data points to that space. This significantly reduces computational complexity because the inner product can be computed in terms of the raw features, which takes $O(\ell + \log p)$ time, where $\ell$ is the dimensionality of the raw feature space and $p$ is the degree of the polynomials. This is much more efficient than the $O(d)$ time it would take to compute the inner product after mapping to the augmented feature space, where $d$ could be exponentially larger than $\ell$. The kernel trick is particularly significant for polynomial feature maps, where the degree of the polynomial $p$ could make the dimensionality of the augmented feature space $d = O(\ell^p)$ impractically large."
"Define the concept of effective receptive field in the context of convolutional neural networks and explain how a stack of $3 \times 3$ convolutional filters can achieve the same effective receptive field as a larger filter. How does the effective receptive field of a stack of three $3 \times 3$ convolutional filters compare to a single $7 \times 7$ filter in terms of the resulting image size and the number of weights required?
","The effective receptive field in convolutional neural networks refers to the size of the region in the input space that a particular feature is looking at, or in other words, the area of the input image that affects the output of the convolutional operation. A stack of $3 \times 3$ convolutional filters can achieve the same effective receptive field as a larger filter by having multiple layers of $3 \times 3$ filters applied sequentially. For example, applying a $3 \times 3$ filter to a $7 \times 7$ image results in a $5 \times 5$ output. If this process is repeated two more times, the output becomes $1 \times 1$, which is the same as applying one $7 \times 7$ filter to the original image. However, a stack of three $3 \times 3$ filters requires $3*(3^2C)$ weights, whereas a single $7 \times 7$ filter requires $7^2C$ weights, where $C$ is the number of channels. Thus, the stack of smaller filters is computationally less expensive and allows for deeper architectures with more non-linearities.

"
"In the context of ResNet architectures, define the concept of a residual block and explain how it addresses the vanishing gradient problem. How does the gradient signal in a residual block compare to a regular neural network when considering the partial derivatives of the output with respect to the input?
","A residual block is a building block of a ResNet architecture that aims to learn a residual function with reference to the layer inputs. The concept is that instead of learning an underlying mapping directly, the network learns the difference, or residual, between the input and output. This is represented as $\mathcal{F}(x):=\mathcal{H}(x)-x$. In a residual block, the network outputs $\mathcal{F}(x)+x=\mathcal{H}(x)$. This approach helps address the vanishing gradient problem by ensuring that the gradient signal does not diminish as it is backpropagated through the network. In regular neural networks, the gradient is calculated as $\frac{\partial L}{\partial x}=\frac{\partial L}{\partial Y} \frac{\partial Y}{\partial x}$, which can be small for deeper layers. In a residual block, the gradient is $\frac{\partial Y}{\partial x}=\frac{\partial F(x)}{\partial x}+1$. The addition of the identity function (the $+1$ term) ensures that there is always a gradient signal being backpropagated, allowing for deeper networks without the vanishing gradient problem.

"
"Analyze the relationship between the depth of neural networks and their capacity as universal function approximators. What are the implications of increased depth on the parameter efficiency and generalization capabilities of deep neural networks compared to shallow networks?
","Both shallow and deep neural networks are universal function approximators, meaning they can approximate any continuous function given sufficient neurons. However, there is empirical and theoretical evidence suggesting that deep neural networks might have an advantage in parameter efficiency, requiring fewer parameters to achieve the same level of approximation as shallow networks, potentially even exponentially fewer. This could be due to the compositional nature of functions learned in deep networks, which can represent complex functions more compactly. Additionally, deep networks are believed to possess better generalization capabilities, meaning they are more likely to perform well on unseen data. This is partly because deep networks can learn hierarchies of features, with more abstract features built upon simpler ones, which can lead to a more robust understanding of the data. The optimization and performance benefits of adding more layers, as well as their implications for generalization, are subjects of ongoing theoretical research in the field of deep learning."
"Define entropy in the context of decision trees and the associated uncertainty it measures. What property of entropy is utilized to choose the best split when constructing a decision tree?
","Entropy, denoted as \(H(Y)\), is used to quantify the uncertainty or impurity in a dataset. It is the expected surprise of observing a discrete random variable \(Y\), calculated as \(H(Y) = -\sum_{k} P(Y=k) \log P(Y=k)\). The property of entropy that is utilized in decision tree construction is its ability to measure the reduction in uncertainty when a split is made. The split that most reduces this uncertainty, or equivalently, that maximizes the information gain, is considered the best split.

"
"Explain the concept of conditional entropy and how it is used to minimize uncertainty within a decision tree. What does minimizing conditional entropy achieve in terms of class distributions after a split?
","Conditional entropy, denoted as \(H(Y|X_{j,v})\), measures the entropy of the class distribution after a split has been made on feature \(j\) at value \(v\). It is defined as a weighted sum of the entropy of each subset created by the split, with weights proportional to the sizes of the subsets: \(H(Y|X_{j,v}) = P(X_{j,v}=1) H(Y|X_{j,v}=1) + P(X_{j,v}=0) H(Y|X_{j,v}=0)\). Minimizing conditional entropy aims to create subsets where the class distributions are less impure (more homogeneous) compared to the original set, thereby reducing the overall uncertainty of the classification.

"
"What is Gini impurity and how is it analogous to entropy in the context of decision trees? Why might Gini impurity be preferred over entropy in some cases?
","Gini impurity is a measure of how often a randomly chosen element from a set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset. It is defined as \(G(Y) = 1 - \sum_{k} P(Y=k)^2\). Analogous to entropy, Gini impurity measures the impurity of a dataset and is used to assess the quality of a split. It is preferred over entropy in some cases because it is slightly faster to compute; it does not involve logarithmic calculations.

"
"Why is the misclassification rate not used as a criterion for splitting in decision trees? Explain using the concept of strict concavity and the behavior of the misclassification rate curve.
","The misclassification rate, given by \(M(Y) = 1 - \max_{k} P(Y=k)\), is not used as a splitting criterion in decision trees because it is insensitive; it may assign the same value to different splits that are not equally good. This is due to the misclassification rate curve not being strictly concave; it has only two unique tangent lines and thus in its linear regions, any convex combination of points on the line will lie on the line as well, yielding zero information gain. Unlike strictly convex functions like entropy, the misclassification rate does not guarantee a positive information gain unless the children's distributions are different from the parent's.

"
"What are some heuristics used to decide when to stop splitting in a decision tree to avoid overfitting? Mention at least two and explain the rationale behind them.
","To prevent overfitting, decision trees may employ heuristics such as limited depth (avoiding splits beyond a certain depth in the tree) and node purity (not splitting if the proportion of training points in a class is high enough). The rationale behind limited depth is to prevent the tree from becoming too complex and fitting noise in the data, while the node purity criterion aims to stop growing the tree when a node is already homogeneous enough, indicating that further splits may not provide additional value and could lead to fitting to the idiosyncrasies of the training data. Other criteria include not splitting if the information gain or increase in purity is negligible, indicating that the split does not significantly improve the model."
"Define the concept of visualizing filters in the context of convolutional neural networks (CNNs) and how is it useful in understanding what the network learns. What is the limitation of this method?
","Visualizing filters refers to the method of displaying the learned weights of the filters in a CNN as images, which can give an idea of what types of features the network learns, such as edge detectors. This method is useful for understanding the initial layers of the network, as they often learn low-level features like edges, textures, or colors. However, this method only works well in the first layer because deeper layers have more abstract representations that are harder to visualize directly.

"
"Describe the concept of visualizing activations and explain how it helps in understanding the depth of a convolutional neural network's responses. Additionally, how does conducting a nearest neighbor search in feature space before a fully connected layer contribute to our understanding of the learned features?
","Visualizing activations involves displaying the activation maps produced by the convolutional layers of a CNN. This provides insight into the sparsity of the responses as the network depth increases and shows which features are being activated by the network at each layer. Conducting a nearest neighbor search in feature space before a fully connected layer helps to determine if the features learned by the CNN are useful. It does so by showing if similar images are located near each other in feature space, indicating a translation-invariant representation. For example, two images of elephants on opposite sides of an image would not be close in pixel space, but if the CNN has learned translation-invariant features, they might be neighbors in feature space.

"
"Explain the principle behind reconstruction by deconvolution in the context of CNNs and what it aims to achieve in understanding network activations.
","Reconstruction by deconvolution is a method where one isolates a particular activation in a CNN and reconstructs the original input image based on that activation alone. The aim is to understand the contribution and importance of that particular activation to the output, by determining the visual patterns or features that cause the neuron to activate. This helps in identifying what part of the input image is responsible for a given activation.

"
"Define activation maximization and its relationship to Hubel and Wiesel's experiment in the field of neural networks. How is the concept applied computationally in CNNs?
","Activation maximization is a technique where one finds the input image that maximizes the activation of a particular neuron or layer within a CNN. This is akin to Hubel and Wiesel's experiments, where they found the specific visual stimuli that maximized the response of neurons in the visual cortex of cats. Computationally, this is done by optimizing an image to produce the highest possible activation for a given neuron, which provides insight into the preferred stimulus of the neuron.

"
"What are saliency maps in the context of CNNs and what purpose do they serve in understanding neural activations?
","Saliency maps are visual representations that highlight the regions of the input image that are most influential in the decision of a CNN. They are computed by identifying which pixels in the image, when changed, would affect the output of the network the most. This allows one to see what locations in the image make a neuron fire and contributes to the understanding of how a CNN is focusing on certain parts of the image to make a classification or prediction.

"
"Describe the technique of code inversion in CNNs and what it seeks to accomplish in terms of feature representation and understanding.
","Code inversion is a technique used to reverse the encoding process of a CNN. Given a feature representation from an intermediate layer of a CNN, code inversion aims to reconstruct the original input image that led to that feature representation. This helps in understanding what types of input patterns correspond to certain features, thereby giving insight into the internal representations of the network.

"
"Explain what is meant by semantic interpretation of activations in CNNs and provide an example of how this interpretation could be useful.
","Semantic interpretation of activations involves understanding the meaning or concept behind the activations within a CNN. It goes beyond visualizing the activations and seeks to interpret them in terms of their semantic content. For example, if a CNN is trying to classify objects in images, a semantic interpretation might involve determining whether the network is using features related to the shininess of objects as part of its classification criteria. Understanding the semantics behind activations can provide deeper insight into the decision-making process of CNNs and can help in the development of more interpretable and explainable AI systems."
"Define the concept of the directional derivative and how it relates to determining the direction of steepest descent for a multivariate function. How is the direction of steepest descent at a point $\mathbf{w}^{(t)}$ in the domain of the function $f$ determined according to the course notes?
","The directional derivative in a unit direction $\mathbf{u}$ at a point $\mathbf{w}^{(t)}$ is defined as the inner product of the gradient at that point and the direction $\mathbf{u}$. Mathematically, it is represented as $D_{\mathbf{u}} f\left(\mathbf{w}^{(t)}\right)=\langle \nabla f\left(\mathbf{w}^{(t)}\right), \mathbf{u}\rangle=\left\|\nabla f\left(\mathbf{w}^{(t)}\right)\right\| \cdot\|\mathbf{u}\| \cdot \cos (\theta)$, where $\theta$ is the angle between $\nabla f\left(\mathbf{w}^{(t)}\right)$ and $\mathbf{u}$. To determine the direction of steepest descent at point $\mathbf{w}^{(t)}$, we find the direction that minimizes the directional derivative, which occurs when $\theta = -\pi$, indicating that $\mathbf{u}$ and $\nabla f\left(\mathbf{w}^{(t)}\right)$ are in opposite directions. Therefore, the direction of steepest descent is $-\nabla f\left(\mathbf{w}^{(t)}\right)$.

"
"Describe the role of the scaling factor $\alpha_{t}$ in the gradient descent algorithm according to the provided course notes. What are the potential consequences of choosing a scaling factor that is either too high or too low?
","The scaling factor $\alpha_{t}$ in the gradient descent algorithm is used to scale the gradient when taking a step in the direction of steepest descent. It determines the size of the step towards the local minimum. Choosing a scaling factor that is too high may cause the algorithm to diverge from the optimal solution, as it can overshoot the minimum. On the other hand, a scaling factor that is too low may cause the algorithm to converge too slowly, as it takes very small steps towards the minimum. The determination of $\alpha_{t}$ is dependent on the attributes of the function $f$ and sometimes requires adaptive adjustment.

"
"Explain the difference between batch gradient descent and stochastic gradient descent (SGD) as given in the course notes. What are the advantages of using SGD over batch gradient descent?
","Batch gradient descent, also referred to as standard gradient descent, computes the full gradient of the objective function $f$ using all $n$ examples in the dataset. This is computationally expensive and can lead to the algorithm getting stuck at local minima due to its deterministic nature. Stochastic gradient descent (SGD) mitigates these issues by using stochastic gradients, which are random vector-valued functions that are equal to the true gradient in expectation. The advantages of SGD include faster gradient updates and the ability to escape local minima and saddle points more easily due to the stochastic nature of the gradients. However, SGD often requires a higher number of iterations to converge.

"
"In the context of the course notes, what is mini-batch gradient descent and how does it differ from batch gradient descent and SGD? What is the impact of the size of the mini-batch on the performance and stability of the algorithm?
","Mini-batch gradient descent is a variant of batch gradient descent that samples and sums a random subset of gradients over $k<n$ indices instead of using the entire batch of $n$ gradients. This makes each iteration more computationally efficient and allows the algorithm to escape local minima more easily compared to batch gradient descent. The size of the mini-batch affects both performance and stability; larger mini-batches lead to more stability, but at a cost of higher computational expenses. Conversely, smaller mini-batches introduce more noise, which can help escape local minima but may also cause instability. Mini-batch gradient descent generally requires a higher number of overall iterations compared to batch gradient descent, but with an appropriate choice of $k$, it can be more computationally efficient overall.

"
"Define Polyak's heavy ball method as outlined in the course notes. How does it improve upon standard gradient descent, and what is the motivation for using a moving average of gradients?
","Polyak's heavy ball method is an optimization technique that introduces a momentum term to the gradient updates, adding inertia to the iterates and helping to prevent deviations from the direction of the updates. It improves upon standard gradient descent by reducing oscillations and potentially accelerating convergence, especially in scenarios where the objective function is disproportionately scaled. The updates use a velocity term $\mathbf{v}^{(t)}$ that represents an exponential moving average of all gradients seen so far. The motivation for using a moving average of the gradients is to downplay directions where the gradient oscillates over time and to boost directions where the gradient is consistent. This helps to accelerate convergence when navigating ""ravines"" in the function landscape by dampening the effects of the gradient in constricted directions."
"Define ensemble learning and explain how averaging is used within this concept to reduce overfitting in machine learning models. Additionally, what is the mathematical expectation of the average of a set of uncorrelated random variables $\left\{Y_{i}\right\}_{i=1}^{n}$ with common mean $\mu$ and variance $\sigma^{2}$?
","Ensemble learning is a general technique in machine learning where the predictions of many varied models are combined into a single prediction to combat overfitting, using a plurality vote for classification or averaging for regression. Mathematically, the expectation of the average of a set of uncorrelated random variables $\left\{Y_{i}\right\}_{i=1}^{n}$ with common mean $\mu$ and variance $\sigma^{2}$ is equal to the common mean $\mu$, as shown by the equation $\mathbb{E}\left[\frac{1}{n} \sum_{i=1}^{n} Y_{i}\right]=\mu$.

"
"Using the laws of variance, derive and explain the variance of the average of the set of uncorrelated random variables $\left\{Y_{i}\right\}_{i=1}^{n}$, and describe how this relates to the variance of individual predictions in the context of ensemble methods.
","The variance of the average of the set of uncorrelated random variables $\left\{Y_{i}\right\}_{i=1}^{n}$ is derived using the laws of variance, which gives us $\operatorname{Var}\left(\frac{1}{n} \sum_{i=1}^{n} Y_{i}\right)=\frac{\sigma^{2}}{n}$. This means that the variance of the combined prediction in ensemble methods is reduced by a factor of $n$ compared to the variance of any individual prediction. This reduction in variance helps to decrease the likelihood of overfitting in the model.

"
"Define random forests as an ensemble method and explain how they reduce correlation among the individual models, which are decision trees.
","Random forests are a specific type of ensemble method where the individual models are decision trees trained in a randomized way to reduce correlation among them. Since the basic decision tree algorithm is deterministic, randomization techniques are applied to produce varied trees even when using the same dataset and hyperparameters. This randomization helps to ensure that the decision trees in the forest are not simply copies of each other, which reduces the correlation between them and, consequently, the variance of the combined prediction."
"Define the terms hyperparameter, model hyperparameter, and optimization hyperparameter, and explain how they differ from the decision variables being optimized when fitting a model to data. What are examples of each type of hyperparameter according to the provided text?
","Hyperparameters are parameters whose values are set before the learning process begins, and they are not determined by the data-fitting optimization procedure. Model hyperparameters determine the structure of the model, such as the model order $d$ in a hypothesis function. Optimization hyperparameters are aspects of the optimization procedure used to fit the model, rather than being a part of the model itself, such as the regularization weight $\lambda$ in ridge regression. Unlike hyperparameters, decision variables are the parameters that the learning algorithm adjusts to minimize some loss function on the data.

"
"Define risk (or true error) and empirical risk (or training error), and explain the difference between them. How do we approximate the true error when we do not have access to the true data distribution?
","Risk, or true error, is the expected loss over the whole data distribution $\mathcal{D}$, measured by a loss function $\ell$, for a particular hypothesis $h$. Empirical risk, or training error, is the average loss over a finite sample of data points from the distribution $\mathcal{D}$. It approximates the true error that we care about but is often biased because it is based on only a sample of the data. To approximate the true error when the true data distribution is not accessible, we use a validation set to estimate the true error by the validation error, which is computed similarly to the empirical risk but on a separate set of data not used for training the model.

"
"Explain the concept of overfitting and how it relates to the addition of features in a linear model. What is the relationship between training error, true error, and model complexity?
","Overfitting occurs when a model fits the training data too closely, capturing noise in the data instead of the underlying true signal. In a linear model, as more features are added, training error can only decrease because the optimizer can always set irrelevant feature weights to zero. However, beyond a certain point, adding more features causes the model to overfit, and the true error, which reflects the model's performance on new, unseen data, begins to increase. Hence, there is a trade-off where initially, model complexity improves true error, but after a certain complexity threshold, it harms the model's ability to generalize, causing true error to rise.

"
"Describe the process of $k$-fold cross-validation and its purpose. What is a drawback of using $k$-fold cross-validation compared to a single validation set?
","In $k$-fold cross-validation, the data is shuffled and partitioned into $k$ equally-sized blocks. The model is trained on all data except one block and then evaluated on the omitted block. This process is repeated for each of the $k$ blocks, and the validation errors are averaged to estimate the true error. The purpose of $k$-fold cross-validation is to use all data points for both training and validation, thus avoiding the ""data incest"" problem and making efficient use of the data. A drawback is that it requires roughly $k$ times as much computation as using a single validation set because the training process must be repeated for each fold.

"
"In the context of the regression framework discussed, explain why the $\ell^2$ norm is used to measure the error of predictions and to penalize model parameters. How do Maximum Likelihood Estimation (MLE) and Maximum a Posteriori (MAP) provide a justification for using the $\ell^2$ norm in regression?
","The $\ell^2$ norm is used in regression to measure the error of predictions and to penalize model parameters because, under the assumption of Gaussian noise, it corresponds to maximizing the likelihood (MLE) or the posterior (MAP) probability of the data given the model. MLE leads to an optimization problem equivalent to Ordinary Least Squares (OLS), which minimizes the sum of the squared errors between the observed outputs and the model's predictions. MAP, when assuming a Gaussian prior on the model parameters, leads to Ridge Regression, which adds a penalized ridge term (equivalent to the $\ell^2$ norm of the parameters) to the OLS objective. Thus, MLE and MAP with Gaussian assumptions provide a statistical justification for using the $\ell^2$ norm in regression."
"Define the line search optimization algorithm and the three steps it involves. What is the main task carried out differently compared to typical gradient descent methods?
","The line search optimization algorithm is an iterative optimization method that repeatedly finds the minimum of a function along a one-dimensional slice, rather than taking small gradient steps. The three steps involved in each iteration of line search are: (1) choosing a promising descent direction or a random direction, (2) looking ahead in that direction to find the minimum, and (3) moving to that minimum. The main task carried out differently is the optimization along a one-dimensional ""sliced"" function, which is simpler than finding the global minimum of multi-dimensional functions.

"
"Define the descent direction in the context of line search optimization and explain the condition it must satisfy. How is this direction typically chosen?
","In the context of line search optimization, a descent direction is any direction that results in a decrease in the function's value, indicated by a negative directional derivative. Mathematically, the condition it must satisfy is \( D_{\mathbf{u}} f\left(\mathbf{w}^{(t)}\right) = \langle \nabla f\left(\mathbf{w}^{(t)}\right), \mathbf{u} \rangle < 0 \). This direction is typically chosen as the negative gradient (as in gradient descent) or by using a coordinate descent approach, where an arbitrary coordinate is selected for the descent.

"
"What is the role of the scalar \( \alpha_{t} \) in the line search algorithm, and how is it determined?
","The scalar \( \alpha_{t} \) in the line search algorithm determines the step size along the descent direction \( \mathbf{u}^{(t)} \). It is determined by minimizing the function \( h(\alpha) = f\left(\mathbf{w}^{(t)} + \alpha \mathbf{u}^{(t)}\right) \), which represents the value of the original function \( f \) along the line determined by \( \mathbf{w}^{(t)} \) and \( \mathbf{u}^{(t)} \). Finding \( \alpha_{t} \) that minimizes \( h(\alpha) \) ensures the algorithm moves towards the minimum in the chosen descent direction.

"
"In the context of line search methods, explain the potential advantages over gradient descent methods. What are the specific scenarios where line search methods can be more beneficial?
","Line search methods offer advantages over gradient descent methods in that they do not necessarily require the calculation of gradients, making them suitable for non-differentiable domains. Additionally, they can be more robust to local minima because they focus on finding the global minima of one-dimensional functions ahead of them, rather than relying on incremental steps that may get trapped in local minima. Scenarios where line search methods can be more beneficial include optimization problems with non-differentiable functions and situations where avoiding local minima is critical."
"Define the chain rule as it relates to the computation of derivatives in the context of neural networks. How does the chain rule facilitate the computation of the gradient of the loss function with respect to a specific weight in the network?
","The chain rule in the context of neural networks is a mathematical tool that allows us to compute the derivative of a composite function. It states that the derivative of the loss function with respect to any given vertex \( v_i \) in the computational graph can be expressed as the sum of the products of the derivatives of the loss with respect to its outgoing neighbors \( v_j \) and the derivatives of those neighbors with respect to \( v_i \):
\[
\frac{\partial \ell}{\partial v_{i}}=\sum_{v_{j} \in \operatorname{out}\left(v_{i}\right)} \frac{\partial \ell}{\partial v_{j}} \frac{\partial v_{j}}{\partial v_{i}}
\]
This facilitates the computation of the gradient of the loss function with respect to a specific weight \( w_i \) because it allows us to break down the computation into simpler parts by considering the influence of each vertex on the loss function and summing these contributions.

"
"Explain how the computational graph of a neural network is structured and how it aids in computing derivatives. What is the significance of having a directed acyclic graph (DAG) for this purpose?
","The computational graph of a neural network is a finite directed acyclic graph \( G=(V, E) \) where each vertex \( v_i \) represents the result of some differentiable computation, and each edge represents a computational dependency. The DAG structure allows for an efficient computation of derivatives because it enables a topological ordering of the vertices, which ensures that derivatives can be calculated in a ""back to front"" order using dynamic programming principles. This means that the derivative of the loss with respect to any vertex \( v_i \) can be computed after all of its outgoing neighbors' derivatives have been calculated, allowing for reuse of computations and thereby reducing redundancy and improving efficiency.

"
"Using the chain rule and the computational graph of a neural network, derive the expression for the derivative of the loss function with respect to a weight \( w_i \). Why is the direct evaluation of this expression considered inefficient?
","The chain rule allows us to express the derivative of the loss function with respect to a weight \( w_i \) as a sum over all paths from \( w_i \) to the loss vertex \( \ell \) in the computational graph:
\[
\frac{\partial \ell}{\partial w_{i}}=\sum_{\text{paths } v^{(1)}, \ldots, v^{(k)} \text{ from } w_{i} \text{ to } \ell} \frac{\partial \ell}{\partial v^{(k)}} \frac{\partial v^{(k)}}{\partial v^{(k-1)}} \cdots \frac{\partial v^{(2)}}{\partial v^{(1)}} \frac{\partial v^{(1)}}{\partial w_{i}}
\]
Direct evaluation of this expression is inefficient because many terms appear in more than one path from \( w_i \) to \( \ell \), resulting in redundant calculations. The computational cost becomes prohibitive especially in large networks with many paths.

"
"Describe how the backpropagation algorithm improves the efficiency of computing the gradient of the loss function with respect to the weights. How does it utilize dynamic programming principles?
","The backpropagation algorithm improves efficiency by combining the chain rule with dynamic programming principles. It computes the derivative of the loss function with respect to each vertex in a ""back to front"" order, which means that it first computes the derivatives for the outgoing neighbors before computing the derivative for the vertex itself. By storing these computed derivatives, the algorithm avoids redundant calculations. Dynamic programming is used to divide the large problem (computing the gradient of the loss function for all weights) into smaller subproblems (computing the individual terms \( \frac{\partial \ell}{\partial w_{i}} \)), solving these subproblems once, storing their solutions, and reusing these stored solutions to efficiently solve the larger problem.

"
"How do you compute the derivative of the loss function with respect to the weights \( w_{ji} \) and activations \( a_i \) in a fully connected layer? What is the significance of these derivatives in the context of backpropagation?
","In a fully connected layer, the derivative of the loss function with respect to the weights \( w_{ji} \) is computed using the activations \( a_i \) from the previous layer as:
\[
\frac{\partial \ell}{\partial w_{j i}}=\frac{\partial \ell}{\partial z_{j}} a_{i}
\]
Similarly, the derivative of the loss function with respect to the activations \( a_i \) is computed as:
\[
\frac{\partial \ell}{\partial a_{i}}=\sum_{j=1}^{k} \frac{\partial \ell}{\partial z_{j}} w_{j i}
\]
These derivatives are significant because they represent the sensitivity of the loss function to changes in the model's parameters (weights) and activations. They are used in backpropagation to update the weights of the network in a way that minimizes the loss function, and they are passed backward to earlier layers to propagate the error signal through the network."
